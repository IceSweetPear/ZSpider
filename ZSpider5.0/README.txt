

	简单易用php爬虫  
	
	此版本 实现了  字符串/正则 匹配    还有  数据 向下传递  回调函数  


	





	处理草书




		现在 访问 nav 的 却是 主页的

		访问 link 却是 nav 的


		因为 开始页用的是 start 

		在循环 nav 时 其实  用的是 start url 

		所以 在对 url 进行 按规则寻找下一层  和 返回 url html 时 都是用的 starturl 

		真正的url 已经保存到 white 中了 

		解决  1  先解析 一遍 start  有点复杂

	
		解释  因为 泽哥的url 规则 是 寻找某个 url 并解析

		而我的是 从父级 url 依照 这个规则 寻找 子级 url 交给 下一级来处理

		
		分页处理
		
		现在的不支持分页的

		要么 融入到代码之中  比较完整 有点复杂  改动较大

		另一种 在配置中加入  此页面是否有分页  然后 去 处理

		比如   在 link 页中  就是 进入 到 nav 的页面中  进入详情页之前  需要把

		分页中的 链接 也一并加入到 white中

		关键  分页在哪里处理比较合适

	start
		在start中获取到nav 链接
	nav
		在 nav 中使用的 nav 的 black 返回的是 content的 链接
	content
		根据 content 链接 
			获取到 具体文章


	解决1  在nav 获取 contentlist 中 判断 是否有分页数据  
	           如果这个页面是有分页数据的就跑到 分页  将分页中的数据 加入到 contentList 中

	
	解决2  增加一个grade 这个 grade 可以有特殊的标识 来表示他是一个分页
		这个grade的位置 在 nav 的下面 作用是
		原来是根据navlist中的 每个nav来获取contentlist
		现在根据 navlist中的 每个nav来获取他们下面的 pagelist
		再根据pageList来 获取 contentList
		
		分页列表可能被折叠
		 
		解决 1  通过计算的方式 获取到 最低的几个 和最高的一个 通过等差数列 来进行获取
			还行 就是 太僵硬 有的可能就不行了
		
		解决 2  通过配置 知道 这是个 分页 后  根据分页截取规则  循环截取 下一页
			也还行 就是 可能有的没有下一页
		两种方式  现在先用循环截取下一页的方式吧 后期碰到  没下一页的开启 等差数列的				

	两种方式   第一种不好控制
		 第二种看起来很不错嘿嘿嘿  但是 产生的 contentList 会非常大 
		日后修改成redis+分批处理



	给你一个 navLInk  返回一个 navpageList
	
	现在已经可以跑了
	但是 有一点慢  然后数据量特别大 就不知道啥时候会崩
	
	日后用redis优化一下速度

	然后 数组到10000就全部转移到redis中  到下一层的循环中 在10000往外取
	具体流程是 数组 长度过长 就将 数组 打包到 redis中清空这个数组 
	
	在 代码 和 redis 中 维持一个 rule grade 数组  明确 这些数据 将在什么时候被取出来

	上级 数据过大  保存数据到 redis 
	执行下级的 时候  当 检查 他的状态 
	如果是 未完成状态 再去检查 上一级的redis 
	如果redis 为空 就标记成已完成的上级
	不为空 就未完成
	未完成的时候 当前的数组小于 100 就搬出来 10000个数据  
	再去检查 redis 数据搬完了没有
	搬完了就 标记redis 完成
	没有 就 数组小于 100 再搬出来 10000个数据





	https://gaokao.chsi.com.cn/sch/search.do

	开始页面 获取到 院校列表
	
	还要加上分页

	https://gaokao.chsi.com.cn/sch/schoolInfoMain--schId-800.dhtml  院校主页

	https://gaokao.chsi.com.cn/sch/listzyjs--schId-800.dhtml   专业介绍 

	使用新加一层 或者 循环拼接的方式 获取到 专业介绍页面



	灰色处理

	在 majorInfo  i按
	通过截取  <h4 class="yxk-second-title">  的方式  0 的 不要  1 本科的 2 专科的

	在分隔到 所有 li   便利 li 如果 li里 没有 href  就 算他是一个专业 


			
	
	现在 获取到 那个 专业详情页 后 直接 获取的 所有专业  再进去 专业内容 

	现在要 在 专业 列表 那里 获取一下 父级 专业  

	解决 1    	

	带货流 
		增加 特殊标识   表示  需要带货  到下一分级使用

		标识 使用 'carry'=>[
				'parent_major_name' => 'div@div'
				'parent_school_name' => '#divdiv#'
			]，

		在循环中 检查这个 链接是否带货   如果带货 就 加入白链中

		[					[
			'www.baodu.com',
			'www.zzzope.com',		=> 		[
			'www.zzzzz.live',					'url'=>'www.baidu.com'，'parent_major_name'=>'文学'
		]						]
			
		  					]
			
		然后 pageinfo 中 需要 $pageinfo['carry'] 就可以获取 上级传来的 数据了

		缺点： 内存翻倍 


	现在准备升级爬虫    1，改进、整合 爬虫结构  确定爬虫方向

		改进整合的话   主要是 数组 url  拼接 那里 太过亢余 分页那边也亢余  整体也亢余

		还有 现在是单线路往下进行的  看看能不能 做成树的形状  再加上完美的进程控制  就完美了

		此爬虫方向暂时为   爬取指定网站的指定信息

	
		树形结构为

		【】
	          【】     【】	
	【】       【】     【】      【】		意思是每个主入口 有 多个 子入口   每个子入口 又有多个子入口   每个子入口 都是一个单独的进程

	
		

	思想 和 架构 都已经有了  怎么具体实现呢


	分页的实现	从主入口进入	现在有多个分页	如果分页为true	进入到分页模式将 urlSub 为截取下一页循环把   多分页 页面 截取下来   传递个下一层使用

	下一层 使用的是  每个分页 	每个分页中 有 多个 schoolList  	再交给下级处理  	现在的模式是  如果 不 是分页 的话 就 根据父级页面 截取到子级 urlList 并发送  一个 子级容器  子级urlList  	现在要添加分页  只是把分页当作一个层级 来处理   根据父级页面  根据截取规则  截取到 下一页的 url 判断 page  如果是分页 就 进入分页的循环中

	
	2.0 基础部分已经成型

	1.0 像竹竿一样 一节一节 的往上走 

	2.0 像大树一样 向外 分支散叶   

	2.0 解决了三个问题  结构松散 不标准     黑白数组 存储量过大     模式无法加载进程 

	2.0 使用了 递归调用的方式   每一个dotask  保存了 要做的事  和 拿什么做事    每个都一样  

	
	某些链接不好直接截取    新加一个功能 ： 模糊截取

	比如一些链接事  <a 'www.zzz.com'> <a "www.zzz.com">   当他们出现在一起的时候  url会带上  “www.xuezhangbb”

	他将 判断截取的链接是否包含  两个 分号   如果包含 就  把第一个分号左边的删掉 第二个分号 右边的删掉		



	2.0 下一步是    参数 传递   多个 回调   @# 截取   封装 获取 header   发送post 


3.0  	现在咱们的 2.0爬虫 并不算是 面向任务式   而是 递归 等待 的一种形式  这种形式很难受

	在递归的时候 所有 父级都在 foreach 那里等待  等一条路线 完全执行完毕之后才释放 极大的占用了内存 

	所以 我想的是 创建一个 任务队列 专门存放 图纸 和 数据  
	
	每个 任务  监听 任务队列  取得设计图 和 数据 并把 设计图 和 数据 存到 任务队列中 

	任务队列 一定要 排队 

	图纸 保存在 一个 图纸数组中 键为 图纸名  

	数据保存在 队列中 队列中 保存了 数据 和 图纸编号

	每个任务 从 队列中 取出一个 图纸编号 和 数据  并取得图纸 去解析这个数据 

	再生成 图纸 和数据 存到 队列中 
	
4.0
	现在虽然能跑 但还是在dotask 的控制下跑的  

	如果不循环调用 dotask = 队列中就没有数据  也就执行不下去了

	
	主进程启动一下后 由 子进程 管理 队列 向 队列中 移除 补充 数据  

	现在的队列 有两种方式

	数组中 -> 需要使用进程通信

	redis中 -> 需要打通redis 且redis需要加进程锁


	我们现在 的操作是 每个数据列表 保存了 任务name 

	把每个获取到到的 子级任务 保存在 队列中

	根据任务name 去拿 任务体

	但是 任务体  中有 闭包  闭包 存不进去 队列

	现在想的是 找个 办法 保存闭包

	找个对象 保存 闭包 再 序列化对象？ 有点难度。可能需要反射等机制

	我们现在 的 任务体放在队列中 是为了 多进程 做准备

	因为现在 的 子级任务体 是 实时获取的   所以 为了让子进程 可以获取和添加 用了redis 

	能不能 在主进程 就 把 任务体 的函数解析好？ 一个foreach 搞定

	一开始就保存好了 任务体

	在 doTask 函数中 就不跟 任务体 传递 或 添加 队列了 

	直接 通过 urlList 中的 任务名 去 解析好的 任务体 数组 中 获取到 自己想要的 任务体

6.0
	加了心心念念的 redis 多进程 可配置  爽啊







